{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled5.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMgvSDfX8f3HXUgGk4Jg0Ji",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hudada369/fy2020-repo-config/blob/master/Pytorch05.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **模型：模型的创建步骤和nn.Module细节学习**"
      ],
      "metadata": {
        "id": "G969RXE8-9lp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**搭建模型的容器Containers 包括 nn.Sequential nn.ModuleList, nn.ModuleDict**\n",
        "\n",
        "\n",
        "1.   Pytorch模型的建立\n",
        "2.   Pytorch的容器\n",
        "1.   AlexNet网络的构建\n",
        "2.   总结回顾\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "AEQ3p4nI_Eqb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "模型的内容：nn.Module\n",
        "\n",
        "1. 模型创建\n",
        "  1. 构建网络层：卷积层 池化层 激活函数层\n",
        "  2. 拼接网络层：Lenet AlexNet ResNet等\n",
        "2. 权值初始化：\n",
        "  1. Xavier Kaiming 均匀分布 正态分布"
      ],
      "metadata": {
        "id": "ISP9ebBJDapG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "通过对Lenet网络的分析，可以得到模型的两大要素\n",
        "1. 构建子模块 例如卷积网络中卷积层 池化层 全连接层\n",
        "2. 拼接子模块（有了子模块，我们把子模块按照一定的顺序，逻辑进行拼接起来得到最终的 LeNet 模型）\n",
        "\n",
        "进行代码的分析，nnModule有Lenet模型，对其查看源码：\n",
        "1. lenet.py 文件，在这里面有个 「LeNet 类，继承了 nn.Module」。并且我们发现在「它的__init__方法里面实现了各个子模块的构建」\n",
        "2. 子模块的拼接是在 forward 里面\n",
        "\n",
        "在模型的概念当中，我们有一个非常重要的概念叫做nn.Module,  我们所有的模型，所有的网络层都是继承于这个类的。\n",
        "class Net(nn.Module):\n",
        "  函数"
      ],
      "metadata": {
        "id": "90Au58ZZD4v1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "nn.Module类 包含在 torch.nn中\n",
        "torch.nn:\n",
        "1. nn.Parameter 张量子类，表示可学习参数，如Weights bias\n",
        "2. nn.Module 所有网络层基类，管理网络属性\n",
        "  1. 属性\n",
        "   1. self._parameters  = OrderedDict() 存储管理属于 nn.Parameter 类的属性，例如权值，偏置这些参数\n",
        "   2. self._buffers= OrderedDict()  存储管理缓冲属性，如 BN 层中的 running_mean，std 等都会存在这里面\n",
        "   3. self._backward_hooks= OrderedDict() 存储管理钩子函数（5 个与 hooks 有关的字典\n",
        "   4. self._forward_pre_hooks= OrderedDict()\n",
        "   5. self._forward_hooks= OrderedDict()\n",
        "   6. self._state_dict_hooks= OrderedDict()\n",
        "   7. self._load_state_dict_pre_hooks= OrderedDict()\n",
        "   8. self._modules = OrderedDict() 存储管理 nn.Module 类， 比如 LeNet 中，会构建子模块，卷积层，池化层，就会存储在 modules 中\n",
        "3. nn.functional 函数具体实现，如卷积 赤化 激活函数\n",
        "4. nn.init \n"
      ],
      "metadata": {
        "id": "EHXkLITr_Esr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "nn.Module创建子模块流程：\n",
        " 先有一个大的 Module 继承 nn.Module 这个基类，比如上面的 LeNet，然后这个大的 Module 里面又可以有很多的子模块，这些子模块同样也是继承于 nn.Module， 在这些 Module 的__init__方法中，会先通过调用父类的初始化方法进行 8 个属性的一个初始化。然后在构建每个子模块的时候，其实分为两步，第一步是初始化，然后被__setattr__这个方法通过判断 value 的类型将其保存到相应的属性（是模型还是参数）字典里面去，然后再进行赋值给相应的成员。这样一个个的构建子模块，最终把整个大的 Module 构建完毕\n",
        "\n",
        "2. nn.Module 进行总结：\n",
        "\n",
        " 1. 一个 module 可以包含多个子 module（LeNet 包含卷积层，池化层，全连接层）\n",
        " 2. 一个 module 相当于一个运算， 必须实现 forward() 函数（从计算图的角度去理解）\n",
        " 3. 每个 module 都有 8 个字典管理它的属性（最常用的就是_parameters，_modules ）"
      ],
      "metadata": {
        "id": "DcLtqQL4KHqS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. 模型容器 Containers\n",
        " 1. nn.Sequential(按顺序包装多个网络层) \n",
        " 2. nn.ModuleList(像list一样包装多个网络层)\n",
        " 3. nn.ModuleDict（像dict一样包装多个网络层）\n",
        " "
      ],
      "metadata": {
        "id": "eJYSJb8oK5AW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "nn.Sequential: 按顺序包装网络层，将深度学习模型分为特征提取部分，全连接层为模型部分\n",
        "\n",
        "1. 从下面看到，分为两部分第一部分features模型 特征提取\n",
        "第二部分 classifier 用于分类\n",
        "2. forward函数，只需要features处理输出，形状变换，然后classilier搞定\n",
        "3. Sequential如何实现子模块拼接 (包括在forward中不同模块的合并)：\n",
        "  1. 在Sequentia定义 的时候，会将每一层子模块的信息存入到_modules 这个有序字典中\n",
        "  2. 前向传播的时候，Sequential的forward函数会便利字典，将 每一层拿出来做数据处理\n",
        "  3. 串行实现，上一层的输出是下一层的输入：\n",
        "    for module in self._module.values():\n",
        "      input = module(input0\n",
        "    return input\n",
        "\n"
      ],
      "metadata": {
        "id": "wsoWzIAsRPOC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " import troch.nn as nn\n",
        " class LeNetSequential(nn.Module):\n",
        "     def __init__(self, classes):\n",
        "         super(LeNetSequential, self).__init__()\n",
        "         self.features = nn.Sequential(\n",
        "             nn.Conv2d(3, 6, 5),\n",
        "             nn.ReLU(),\n",
        "             nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "             nn.Conv2d(6, 16, 5),\n",
        "             nn.ReLU(),\n",
        "             nn.MaxPool2d(kernel_size=2, stride=2),)\n",
        " \n",
        "         self.classifier = nn.Sequential(\n",
        "             nn.Linear(16*5*5, 120),\n",
        "             nn.ReLU(),\n",
        "             nn.Linear(120, 84),\n",
        "             nn.ReLU(),\n",
        "             nn.Linear(84, classes),)\n",
        " \n",
        "     def forward(self, x):\n",
        "         x = self.features(x)\n",
        "         x = x.view(x.size()[0], -1)\n",
        "         x = self.classifier(x)\n",
        "         return x"
      ],
      "metadata": {
        "id": "CzVQv2XhSPgT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "通过对上面构建的网络层观察：子模块的字典键成了序号，如果网络层很多的话，很难通过对序号进行索引，所以可以对网络层进行命名，也就是：\n",
        " 1. 在 nn.Sequential(OrderedDict({\n",
        "   'cov1':nn.Cov2d(3,6,5)\n",
        " }))\n",
        " 2. 有序的字典，字典中是网络名:网络层的形式(在代码运行的时候，container的init函数中就会判断这个网络是不是有序字典的形式，不是的话就按照序号编码)\n",
        "\n",
        "\n",
        "nn.Sequential的总结：\n",
        " 1. 顺序性：各网络层之间严格按照顺序构建，这时候一定要注意前后层数据的关系\n",
        " 2. 自带 forward(): 自答的 forward 里，通过 for 循环依次执行前向传播运算"
      ],
      "metadata": {
        "id": "WDAlLv-sZQGe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LeNetSequentialOrderDict(nn.Module):\n",
        "     def __init__(self, classes):\n",
        "         super(LeNetSequentialOrderDict, self).__init__()\n",
        " \n",
        "         self.features = nn.Sequential(OrderedDict({\n",
        "             'conv1': nn.Conv2d(3, 6, 5),\n",
        "             'relu1': nn.ReLU(inplace=True),\n",
        "             'pool1': nn.MaxPool2d(kernel_size=2, stride=2),\n",
        " \n",
        "             'conv2': nn.Conv2d(6, 16, 5),\n",
        "             'relu2': nn.ReLU(inplace=True),\n",
        "             'pool2': nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "         }))\n",
        " \n",
        "         self.classifier = nn.Sequential(OrderedDict({\n",
        "             'fc1': nn.Linear(16*5*5, 120),\n",
        "             'relu3': nn.ReLU(),\n",
        " \n",
        "             'fc2': nn.Linear(120, 84),\n",
        "             'relu4': nn.ReLU(inplace=True),\n",
        " \n",
        "             'fc3': nn.Linear(84, classes),\n",
        "         }))\n",
        " \n",
        "     def forward(self, x):\n",
        "         x = self.features(x)\n",
        "         x = x.view(x.size()[0], -1)\n",
        "         x = self.classifier(x)\n",
        "         return x"
      ],
      "metadata": {
        "id": "lx9JSTGLZ3JO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.2 nn.ModuleList 包装一组网络容器，以迭代方式调用网络层\n",
        " 1. append(): 在 ModuleList 后面添加网络层\n",
        " 2. extend(): 拼接两个 ModuleList\n",
        " 3. insert(): 指定在 ModuleList 中位置插入网络层\n",
        "类似于我们的列表，只不过元素换成网络层\n"
      ],
      "metadata": {
        "id": "FdWbi96PbUg-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ModuleList(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ModuleList, self).__init__()\n",
        "        self.linears = nn.ModuleList([nn.Linear(10, 10) for i in range(20)])\n",
        "\n",
        "    def forward(self, x):\n",
        "        for i, linear in enumerate(self.linears):\n",
        "            x = linear(x)\n",
        "        return x\n",
        "# ModuleList 构建网络层就可以使用列表生成式构建，然后前向传播的时候也是遍历每一层，\n",
        "# 进行计算即可"
      ],
      "metadata": {
        "id": "lzsYdKPWb-es"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "3.3 nn.ModuleDict 包装一组网络层，以 索引的方式 调用网络层，主要方法\n",
        "  1. clear(): 清空 ModuleDict\n",
        "  2. items(): 返回可迭代的键值对(key-value pairs)\n",
        "  3. keys(): 返回字典的键(key)\n",
        "  4. values(): 返回字典的值(value)\n",
        "  5. pop(): 返回一对键值对，并从字典中删除\n",
        "# 从以下看出在选择网络层的时候比较实用，例如在做某些任务的时候\n",
        "# 可以做对比"
      ],
      "metadata": {
        "id": "XQf8NXHacs2S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ModuleDict(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ModuleDict, self).__init__()\n",
        "        self.choices = nn.ModuleDict({\n",
        "            'conv': nn.Conv2d(10, 10, 3),\n",
        "            'pool': nn.MaxPool2d(3)\n",
        "        })\n",
        "\n",
        "        self.activations = nn.ModuleDict({\n",
        "            'relu': nn.ReLU(),\n",
        "            'prelu': nn.PReLU()\n",
        "        })\n",
        "\n",
        "    def forward(self, x, choice, act):\n",
        "        x = self.choices[choice](x)\n",
        "        x = self.activations[act](x)\n",
        "        return x\n",
        "    \n",
        "net = ModuleDict()\n",
        "fake_img = torch.randn((4, 10, 32, 32))\n",
        "output = net(fake_img, 'conv', 'relu')    # 在这里可以选择我们的层进行组合\n",
        "print(output)\n",
        "\n",
        "# 前面通过self.choices这个 ModuleDict 可以选择卷积或者池化，\n",
        "# 而下面通过self.activations这个 ModuleDict 可以选取是用哪个激活函数"
      ],
      "metadata": {
        "id": "x3cFLulndTNa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. nn.Sequential 顺序性 各网络层之间按照顺序执行，用于block构建\n",
        "2. nn.ModuleList 迭代性 常用语大量重复网构建，for循环实现\n",
        "3. nn.ModuleDict 用于可选择的网络层"
      ],
      "metadata": {
        "id": "c4OW9OsieQfc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class AlexNet(nn.Module):\n",
        "\n",
        "    def __init__(self, num_classes=1000):\n",
        "        super(AlexNet, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "        )\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(256 * 6 * 6, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(4096, num_classes),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.classifier(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "C_PmAq9LfbpW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sequential 进行搭建的，分三部分，第一部分是一个 Sequential，由一系列的卷积池化模块构成，目的是提取图像的特征，然后是一个全局的池化层把特征进行整合，最后有一个 Sequential 是全连接层组成的，用于模型的分类。这样就完成了 AlexNet 网络的搭建，forward 函数汇总也是非常简单了"
      ],
      "metadata": {
        "id": "AQFtdgB6fe8y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "olWTbHhfdjQI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "5n79YyfEdizw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}